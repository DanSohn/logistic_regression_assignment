{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CHDModel",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWRdlETIKb9r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b46a2192-4d56-46bc-953e-9045388b6db1"
      },
      "source": [
        "try:\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlYVnZzmKuNH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "import functools\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOW9qvBlK1u7",
        "colab_type": "code",
        "outputId": "e23386d5-5752-402e-98cd-15f7b1a9e347",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-267bc644-d862-49e4-865a-a427eacde0a4\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-267bc644-d862-49e4-865a-a427eacde0a4\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving heart_test.csv to heart_test.csv\n",
            "Saving heart_train.csv to heart_train.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7er0R0jK2qU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_file_path = \"heart_train.csv\"\n",
        "test_file_path = \"heart_test.csv\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zA50VtlAK24c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Make numpy values easier to read.\n",
        "np.set_printoptions(precision=3, suppress=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hi_K8alMK2_b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "9837f3ef-db85-4bea-bcca-d27670496e09"
      },
      "source": [
        "!head {train_file_path}"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "row.names,sbp,tobacco,ldl,adiposity,famhist,typea,obesity,alcohol,age,chd\r\n",
            "1,160,12,5.73,23.11,Present,49,25.3,97.2,52,1\r\n",
            "2,144,0.01,4.41,28.61,Absent,55,28.87,2.06,63,1\r\n",
            "3,118,0.08,3.48,32.28,Present,52,29.14,3.81,46,0\r\n",
            "4,170,7.5,6.41,38.03,Present,51,31.99,24.26,58,1\r\n",
            "5,134,13.6,3.5,27.78,Present,60,25.99,57.34,49,1\r\n",
            "6,132,6.2,6.47,36.21,Present,62,30.77,14.14,45,0\r\n",
            "7,142,4.05,3.38,16.2,Absent,59,20.81,2.62,38,0\r\n",
            "8,114,4.08,4.59,14.6,Present,62,23.11,6.72,58,1\r\n",
            "9,114,0,3.83,19.4,Present,49,24.86,2.49,29,0\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWoQUlBrK3HM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The only column you need to identify explicitly is the one with the value that the model is intended to predict.\n",
        "LABEL_COLUMN = 'chd'\n",
        "LABELS = [0, 1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wy9ad8f_LZfu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "9aade4cd-23ad-4133-d6eb-f6c54da985a1"
      },
      "source": [
        "# read the CSV data from the file and create two datasets for training and testing\n",
        "\n",
        "def get_dataset(file, **kwargs):\n",
        "  dataset = tf.data.experimental.make_csv_dataset(\n",
        "      file,\n",
        "      batch_size = 50,\n",
        "      label_name=LABEL_COLUMN,\n",
        "      na_value =\"?\",\n",
        "      num_epochs=1,\n",
        "      ignore_errors=True,\n",
        "      **kwargs)\n",
        "  return dataset\n",
        "\n",
        "info = ['sbp','tobacco','ldl','adiposity','famhist', 'typea','obesity','alcohol','age','chd']\n",
        "raw_train_data = get_dataset(train_file_path, select_columns=info)\n",
        "raw_test_data = get_dataset(test_file_path, select_columns=info)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-2.0.0/python3.6/tensorflow_core/python/data/experimental/ops/readers.py:521: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_OSJd7TLZkT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def show_batch(dataset):\n",
        "  for batch, label in dataset.take(1):\n",
        "    for key, value in batch.items():\n",
        "      print(\"{:20s}: {}\".format(key,value.numpy()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5emVKxoALZo0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        },
        "outputId": "f42f44cd-e5f1-4f9c-b928-56f2f8461481"
      },
      "source": [
        "show_batch(raw_train_data)\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sbp                 : [128 134 118 144 170 138 122 168 134 114 129 120 124 128 162 136 114 140\n",
            " 158 116 152 117 112 143 122 148 118 136 101 170 164 132 114 106 128 110\n",
            " 132 122 126 114 128 154 144 180 120 138 162 128 140 134]\n",
            "tobacco             : [ 2.6   1.5   0.75  0.04  2.6   1.15  0.    9.    0.9   0.1   2.15  1.4\n",
            "  0.    0.5   5.3  11.2   4.08  8.   13.5   1.91 12.18  1.53  9.65  0.46\n",
            "  3.2   0.    0.12  3.46  0.48  0.    0.5  12.3   0.    5.6   0.7   2.35\n",
            "  0.72  0.    8.75  1.2   0.    4.5   2.4   3.57 10.5   0.    5.6   1.6\n",
            "  3.9   3.  ]\n",
            "ldl                 : [4.94 3.73 2.58 3.38 7.22 5.09 3.76 8.53 3.18 3.95 5.17 6.25 2.28 3.7\n",
            " 7.95 5.81 4.59 4.42 5.04 7.56 4.04 2.44 2.29 2.4  3.59 4.66 3.26 6.38\n",
            " 7.26 3.12 6.95 5.96 4.97 3.2  4.9  3.36 4.37 5.49 6.53 3.98 6.34 4.75\n",
            " 8.13 3.57 2.7  2.68 4.24 5.41 7.32 3.17]\n",
            "adiposity           : [21.36 21.53 20.25 23.61 28.69 27.87 24.59 24.48 23.66 15.89 27.57 20.47\n",
            " 24.86 12.81 33.58 31.85 14.6  33.15 30.79 26.45 37.83 28.95 17.2  22.87\n",
            " 22.49 24.39 12.26 32.25 13.   37.15 39.64 32.79  9.69 12.3  37.42 26.72\n",
            " 19.54 19.56 34.02 14.9  11.87 23.52 35.61 36.1  29.87 17.04 22.53 29.3\n",
            " 25.05 17.91]\n",
            "famhist             : [b'Absent' b'Absent' b'Absent' b'Absent' b'Present' b'Present' b'Absent'\n",
            " b'Present' b'Present' b'Present' b'Absent' b'Absent' b'Present'\n",
            " b'Present' b'Present' b'Present' b'Present' b'Present' b'Absent'\n",
            " b'Present' b'Present' b'Present' b'Present' b'Absent' b'Present'\n",
            " b'Absent' b'Absent' b'Present' b'Absent' b'Absent' b'Present' b'Present'\n",
            " b'Absent' b'Absent' b'Present' b'Present' b'Absent' b'Absent' b'Absent'\n",
            " b'Absent' b'Absent' b'Present' b'Absent' b'Absent' b'Present' b'Absent'\n",
            " b'Absent' b'Absent' b'Absent' b'Absent']\n",
            "typea               : [61 41 59 30 71 61 56 69 52 57 52 60 50 66 58 75 62 47 54 52 63 35 54 62\n",
            " 45 50 55 43 50 47 47 57 26 49 72 54 48 57 49 49 57 43 46 36 54 42 29 68\n",
            " 47 35]\n",
            "obesity             : [21.3  24.7  24.46 23.75 27.87 25.65 24.36 26.18 23.26 20.31 25.42 25.85\n",
            " 22.24 21.25 36.06 27.68 23.11 32.77 24.79 30.01 34.57 25.89 23.53 29.17\n",
            " 24.96 25.26 22.65 28.73 19.82 35.42 41.76 30.12 22.6  20.29 35.94 26.08\n",
            " 26.11 23.12 30.25 23.79 23.14 25.76 27.38 26.7  24.5  22.16 22.91 29.38\n",
            " 27.36 26.37]\n",
            "alcohol             : [  0.    11.11   0.     4.66  37.65   2.34   0.     4.63  27.36  17.14\n",
            "   2.06   8.51   8.26  22.73   8.23  22.94   6.72  66.86  21.5    3.6\n",
            "   4.17  30.03   0.68  15.43  36.17   4.03   0.     3.13   5.19   0.\n",
            "   3.81  21.5    0.     0.     3.09 109.8   49.37  14.02   0.    25.82\n",
            "   0.     0.    13.37  19.95  16.46   0.     5.66  23.97  36.77  15.12]\n",
            "age                 : [31 30 32 30 56 44 30 54 58 16 39 28 38 28 48 58 58 44 62 33 64 46 53 29\n",
            " 58 27 16 43 16 53 46 62 25 39 49 58 28 27 41 26 17 53 60 64 49 16 60 32\n",
            " 32 27]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAfD3eTxLZwA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_batch, labels_batch = next(iter(raw_test_data))\n",
        "train_batch, labels_batch = next(iter(raw_train_data)) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l985UYKzLZb9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# simple function that packs together all the columns\n",
        "def pack(features, label):\n",
        "  return tf.stack(list(features.values()), axis=-1), label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVAktD0lNbd0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define a more general preprocessor that selects a list of numeric features and packs them into a single column:\n",
        "class PackNumericFeatures(object):\n",
        "  def __init__(self, names):\n",
        "    self.names = names\n",
        "\n",
        "  def __call__(self, features, labels):\n",
        "    numeric_features = [features.pop(name) for name in self.names]\n",
        "    numeric_features = [tf.cast(feat, tf.float32) for feat in numeric_features]\n",
        "    numeric_features = tf.stack(numeric_features, axis=-1)\n",
        "    features['numeric'] = numeric_features\n",
        "\n",
        "    return features, labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSSqD8UoNboC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NUMERIC_FEATURES = ['sbp','tobacco','ldl','adiposity', 'typea','obesity','alcohol','age']\n",
        "\n",
        "packed_train_data = raw_train_data.map(\n",
        "    PackNumericFeatures(NUMERIC_FEATURES))\n",
        "\n",
        "packed_test_data = raw_test_data.map(\n",
        "    PackNumericFeatures(NUMERIC_FEATURES))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u14WLDVYNbqo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8dfc6a4d-5cb5-4c46-ddee-feeba26404cf"
      },
      "source": [
        "show_batch(packed_train_data)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "famhist             : [b'Absent' b'Present' b'Present' b'Present' b'Absent' b'Absent' b'Absent'\n",
            " b'Present' b'Absent' b'Absent' b'Absent' b'Absent' b'Present' b'Absent'\n",
            " b'Absent' b'Present' b'Present' b'Absent' b'Present' b'Absent' b'Present'\n",
            " b'Absent' b'Absent' b'Absent' b'Absent' b'Present' b'Absent' b'Absent'\n",
            " b'Absent' b'Present' b'Absent' b'Absent' b'Present' b'Absent' b'Absent'\n",
            " b'Present' b'Absent' b'Absent' b'Absent' b'Present' b'Present' b'Absent'\n",
            " b'Absent' b'Present' b'Absent' b'Present' b'Absent' b'Present' b'Present'\n",
            " b'Present']\n",
            "numeric             : [[108.     0.     2.74  11.17  53.    22.61   0.95  20.  ]\n",
            " [118.     0.08   3.48  32.28  52.    29.14   3.81  46.  ]\n",
            " [158.     2.6    7.46  34.07  61.    29.3   53.28  62.  ]\n",
            " [132.     2.     2.7   21.57  50.    27.95   9.26  37.  ]\n",
            " [120.     1.4    6.25  20.47  60.    25.85   8.51  28.  ]\n",
            " [118.     0.     3.67  12.13  51.    19.15   0.6   15.  ]\n",
            " [118.     0.     2.39  12.13  49.    18.46   0.26  17.  ]\n",
            " [162.     7.4    8.55  24.65  64.    25.71   5.86  58.  ]\n",
            " [166.     0.07   4.03  29.29  53.    28.37   0.    27.  ]\n",
            " [136.     1.2    2.78   7.12  52.    22.51   3.41  27.  ]\n",
            " [156.     0.     3.47  21.1   73.    28.4    0.    36.  ]\n",
            " [124.     4.2    2.94  27.59  50.    30.31  85.06  30.  ]\n",
            " [146.     7.5    7.21  25.93  55.    22.51   0.51  42.  ]\n",
            " [101.     0.48   7.26  13.    50.    19.82   5.19  16.  ]\n",
            " [127.     0.     2.81  15.7   42.    22.03   1.03  17.  ]\n",
            " [132.     6.2    6.47  36.21  62.    30.77  14.14  45.  ]\n",
            " [130.     2.78   4.89   9.39  63.    19.3   17.47  25.  ]\n",
            " [190.     4.18   5.05  24.83  45.    26.09  82.85  41.  ]\n",
            " [124.     6.     5.21  33.02  64.    29.37   7.61  58.  ]\n",
            " [154.     4.2    5.59  25.02  58.    25.02   1.54  43.  ]\n",
            " [108.     0.4    5.91  22.92  57.    25.72  72.    39.  ]\n",
            " [160.    14.     5.9   37.12  58.    33.87   3.52  54.  ]\n",
            " [114.     0.     2.63   9.69  45.    17.89   0.    16.  ]\n",
            " [164.    12.     3.91  19.59  51.    23.44  19.75  39.  ]\n",
            " [144.     6.75   5.45  29.81  53.    25.62  26.23  43.  ]\n",
            " [160.     4.2    6.76  37.99  61.    32.91   3.09  54.  ]\n",
            " [108.     0.8    2.47  17.53  47.    22.18   0.    55.  ]\n",
            " [148.     4.5   10.49  33.27  50.    25.92   2.06  53.  ]\n",
            " [118.     0.12   1.96  20.31  37.    20.01   2.42  18.  ]\n",
            " [114.     0.     3.83  19.4   49.    24.86   2.49  29.  ]\n",
            " [132.     7.     3.2   23.26  77.    23.64  23.14  49.  ]\n",
            " [148.    12.2    3.79  34.15  57.    26.38  14.4   57.  ]\n",
            " [140.     0.     5.08  27.33  41.    27.83   1.25  38.  ]\n",
            " [150.     0.18   4.14  14.4   53.    23.43   7.71  44.  ]\n",
            " [128.     0.     6.34  11.87  57.    23.14   0.    17.  ]\n",
            " [170.     7.6    5.5   37.83  42.    37.41   6.17  54.  ]\n",
            " [122.     0.     3.08  16.3   43.    22.13   0.    16.  ]\n",
            " [148.     4.04   3.99  20.69  60.    27.78   1.75  28.  ]\n",
            " [120.     3.7    4.02  39.66  61.    30.57   0.    64.  ]\n",
            " [124.     4.82   3.24  21.1   48.    28.49   8.42  30.  ]\n",
            " [134.     4.8    6.58  29.89  55.    24.73  23.66  63.  ]\n",
            " [126.    19.6    6.03  34.99  49.    26.99  55.89  44.  ]\n",
            " [128.     5.4    2.36  12.98  51.    18.36   6.69  61.  ]\n",
            " [116.     1.91   7.56  26.45  52.    30.01   3.6   33.  ]\n",
            " [148.     6.     6.49  26.47  48.    24.7    0.    55.  ]\n",
            " [116.     4.28   7.02  19.99  68.    23.31   0.    52.  ]\n",
            " [126.    10.5    4.49  17.33  67.    19.37   0.    49.  ]\n",
            " [134.    11.79   4.01  26.57  38.    21.79  38.88  61.  ]\n",
            " [130.     0.     2.82  19.63  70.    24.86   0.    29.  ]\n",
            " [138.     2.     5.11  31.4   49.    27.25   2.06  64.  ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1mLONY6Nbw3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_batch, label_batch = next(iter(packed_train_data))\n",
        "test_batch, label_batch = next(iter(packed_test_data))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIvHJTAQNb0p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "0821b977-317d-4a03-bd2f-9a03b284ad61"
      },
      "source": [
        "# data normalization\n",
        "\n",
        "import pandas as pd\n",
        "desc = pd.read_csv(train_file_path)[NUMERIC_FEATURES].describe()\n",
        "desc"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sbp</th>\n",
              "      <th>tobacco</th>\n",
              "      <th>ldl</th>\n",
              "      <th>adiposity</th>\n",
              "      <th>typea</th>\n",
              "      <th>obesity</th>\n",
              "      <th>alcohol</th>\n",
              "      <th>age</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>398.000000</td>\n",
              "      <td>398.000000</td>\n",
              "      <td>398.000000</td>\n",
              "      <td>398.000000</td>\n",
              "      <td>398.000000</td>\n",
              "      <td>398.000000</td>\n",
              "      <td>398.000000</td>\n",
              "      <td>398.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>137.484925</td>\n",
              "      <td>3.669020</td>\n",
              "      <td>4.776055</td>\n",
              "      <td>25.428291</td>\n",
              "      <td>53.386935</td>\n",
              "      <td>26.145804</td>\n",
              "      <td>16.936206</td>\n",
              "      <td>43.198492</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>19.385614</td>\n",
              "      <td>4.490931</td>\n",
              "      <td>2.025025</td>\n",
              "      <td>7.664637</td>\n",
              "      <td>9.954609</td>\n",
              "      <td>4.222120</td>\n",
              "      <td>24.642372</td>\n",
              "      <td>14.410089</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>101.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.980000</td>\n",
              "      <td>7.120000</td>\n",
              "      <td>13.000000</td>\n",
              "      <td>17.750000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>15.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>124.000000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>3.272500</td>\n",
              "      <td>19.805000</td>\n",
              "      <td>47.000000</td>\n",
              "      <td>23.222500</td>\n",
              "      <td>0.510000</td>\n",
              "      <td>32.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>134.000000</td>\n",
              "      <td>2.085000</td>\n",
              "      <td>4.400000</td>\n",
              "      <td>26.210000</td>\n",
              "      <td>54.000000</td>\n",
              "      <td>25.830000</td>\n",
              "      <td>7.710000</td>\n",
              "      <td>45.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>148.000000</td>\n",
              "      <td>5.575000</td>\n",
              "      <td>5.900000</td>\n",
              "      <td>31.212500</td>\n",
              "      <td>60.000000</td>\n",
              "      <td>28.470000</td>\n",
              "      <td>23.395000</td>\n",
              "      <td>56.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>218.000000</td>\n",
              "      <td>31.200000</td>\n",
              "      <td>15.330000</td>\n",
              "      <td>42.490000</td>\n",
              "      <td>78.000000</td>\n",
              "      <td>46.580000</td>\n",
              "      <td>147.190000</td>\n",
              "      <td>64.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              sbp     tobacco         ldl  ...     obesity     alcohol         age\n",
              "count  398.000000  398.000000  398.000000  ...  398.000000  398.000000  398.000000\n",
              "mean   137.484925    3.669020    4.776055  ...   26.145804   16.936206   43.198492\n",
              "std     19.385614    4.490931    2.025025  ...    4.222120   24.642372   14.410089\n",
              "min    101.000000    0.000000    0.980000  ...   17.750000    0.000000   15.000000\n",
              "25%    124.000000    0.100000    3.272500  ...   23.222500    0.510000   32.000000\n",
              "50%    134.000000    2.085000    4.400000  ...   25.830000    7.710000   45.000000\n",
              "75%    148.000000    5.575000    5.900000  ...   28.470000   23.395000   56.000000\n",
              "max    218.000000   31.200000   15.330000  ...   46.580000  147.190000   64.000000\n",
              "\n",
              "[8 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9svBfDlNb43",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MEAN = np.array(desc.T['mean'])\n",
        "STD = np.array(desc.T['std'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMu--96zNb9B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalize_numeric_data(data, mean, std):\n",
        "  # Center the data\n",
        "  return (data-mean)/std"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MryQnSrJNcAY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "941d501f-bb19-47c9-9545-5f4871e98559"
      },
      "source": [
        "# See what you just created.\n",
        "normalizer = functools.partial(normalize_numeric_data, mean=MEAN, std=STD)\n",
        "\n",
        "numeric_column = tf.feature_column.numeric_column('numeric', normalizer_fn=normalizer, shape=[len(NUMERIC_FEATURES)])\n",
        "numeric_columns = [numeric_column]\n",
        "numeric_column"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NumericColumn(key='numeric', shape=(8,), default_value=None, dtype=tf.float32, normalizer_fn=functools.partial(<function normalize_numeric_data at 0x7f8874568d08>, mean=array([137.485,   3.669,   4.776,  25.428,  53.387,  26.146,  16.936,\n",
              "        43.198]), std=array([19.386,  4.491,  2.025,  7.665,  9.955,  4.222, 24.642, 14.41 ])))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYVerR0wNb8E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 901
        },
        "outputId": "6f7b48f4-3471-45a7-d1f8-78476aa6f61c"
      },
      "source": [
        "train_batch['numeric']"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: id=266, shape=(50, 8), dtype=float32, numpy=\n",
              "array([[154.  ,   0.  ,   4.81,  28.11,  56.  ,  25.67,  75.77,  59.  ],\n",
              "       [142.  ,   0.28,   1.8 ,  21.03,  57.  ,  23.65,   2.93,  33.  ],\n",
              "       [118.  ,   0.  ,   2.39,  12.13,  49.  ,  18.46,   0.26,  17.  ],\n",
              "       [120.  ,   0.  ,   3.68,  12.24,  51.  ,  20.52,   0.51,  20.  ],\n",
              "       [118.  ,   1.5 ,   5.38,  25.84,  64.  ,  28.63,   3.89,  29.  ],\n",
              "       [112.  ,   1.44,   2.71,  22.92,  59.  ,  24.81,   0.  ,  52.  ],\n",
              "       [122.  ,   1.  ,   5.88,  34.81,  69.  ,  31.27,  15.94,  40.  ],\n",
              "       [116.  ,   4.28,   7.02,  19.99,  68.  ,  23.31,   0.  ,  52.  ],\n",
              "       [114.  ,   1.2 ,   3.98,  14.9 ,  49.  ,  23.79,  25.82,  26.  ],\n",
              "       [143.  ,   5.04,   4.86,  23.59,  58.  ,  24.69,  18.72,  42.  ],\n",
              "       [136.  ,   1.36,   3.16,  14.97,  56.  ,  24.98,   7.3 ,  24.  ],\n",
              "       [164.  ,  13.02,   6.26,  29.38,  47.  ,  22.75,  37.03,  54.  ],\n",
              "       [136.  ,   0.  ,   2.28,  18.14,  55.  ,  22.59,   0.  ,  17.  ],\n",
              "       [134.  ,   1.5 ,   3.73,  21.53,  41.  ,  24.7 ,  11.11,  30.  ],\n",
              "       [142.  ,   2.4 ,   2.55,  23.89,  54.  ,  26.09,  59.14,  37.  ],\n",
              "       [162.  ,   2.92,   3.63,  31.33,  62.  ,  31.59,  18.51,  42.  ],\n",
              "       [134.  ,   8.8 ,   7.41,  26.84,  35.  ,  29.44,  29.52,  60.  ],\n",
              "       [108.  ,   1.5 ,   4.33,  24.99,  66.  ,  22.29,  21.6 ,  61.  ],\n",
              "       [134.  ,   8.08,   1.55,  17.5 ,  56.  ,  22.65,  66.65,  31.  ],\n",
              "       [146.  ,   0.  ,   6.62,  25.69,  60.  ,  28.07,   8.23,  63.  ],\n",
              "       [208.  ,   7.4 ,   7.41,  32.03,  50.  ,  27.62,   7.85,  57.  ],\n",
              "       [218.  ,  11.2 ,   2.77,  30.79,  38.  ,  24.86,  90.93,  48.  ],\n",
              "       [166.  ,   0.6 ,   2.42,  34.03,  53.  ,  26.96,  54.  ,  60.  ],\n",
              "       [118.  ,   0.  ,   4.34,  30.12,  52.  ,  32.18,   3.91,  46.  ],\n",
              "       [116.  ,   3.  ,   3.05,  30.31,  41.  ,  23.63,   0.86,  44.  ],\n",
              "       [134.  ,   0.  ,   5.9 ,  30.84,  49.  ,  29.16,   0.  ,  55.  ],\n",
              "       [134.  ,   0.02,   2.8 ,  18.84,  45.  ,  24.82,   0.  ,  17.  ],\n",
              "       [128.  ,   0.  ,   2.98,  12.59,  65.  ,  20.74,   2.06,  19.  ],\n",
              "       [130.  ,  18.  ,   4.13,  27.43,  54.  ,  27.44,   0.  ,  51.  ],\n",
              "       [128.  ,   0.5 ,   3.7 ,  12.81,  66.  ,  21.25,  22.73,  28.  ],\n",
              "       [130.  ,   2.61,   2.72,  22.99,  51.  ,  26.29,  13.37,  51.  ],\n",
              "       [134.  ,   2.  ,   3.66,  14.69,  52.  ,  21.03,   2.06,  37.  ],\n",
              "       [134.  ,  13.6 ,   3.5 ,  27.78,  60.  ,  25.99,  57.34,  49.  ],\n",
              "       [124.  ,   4.  ,  12.42,  31.29,  54.  ,  23.23,   2.06,  42.  ],\n",
              "       [136.  ,   7.36,   2.19,  28.11,  61.  ,  25.  ,  61.71,  54.  ],\n",
              "       [120.  ,   7.5 ,  15.33,  22.  ,  60.  ,  25.31,  34.49,  49.  ],\n",
              "       [134.  ,  10.  ,   3.79,  34.72,  42.  ,  28.33,  28.8 ,  52.  ],\n",
              "       [140.  ,   0.  ,   5.08,  27.33,  41.  ,  27.83,   1.25,  38.  ],\n",
              "       [116.  ,   2.7 ,   3.69,  13.52,  55.  ,  21.13,  18.51,  32.  ],\n",
              "       [114.  ,   0.  ,   8.01,  21.64,  66.  ,  25.51,   2.49,  16.  ],\n",
              "       [124.  ,   4.2 ,   2.94,  27.59,  50.  ,  30.31,  85.06,  30.  ],\n",
              "       [124.  ,   1.04,   2.84,  16.42,  46.  ,  20.17,   0.  ,  61.  ],\n",
              "       [188.  ,   0.  ,   5.47,  32.44,  71.  ,  28.99,   7.41,  50.  ],\n",
              "       [206.  ,   6.  ,   2.95,  32.27,  72.  ,  26.81,  56.06,  60.  ],\n",
              "       [124.  ,   0.  ,   4.79,  34.71,  49.  ,  26.09,   9.26,  47.  ],\n",
              "       [132.  ,   6.  ,   5.97,  25.73,  66.  ,  24.18, 145.29,  41.  ],\n",
              "       [142.  ,  18.2 ,   4.34,  24.38,  61.  ,  26.19,   0.  ,  50.  ],\n",
              "       [134.  ,   4.8 ,   6.58,  29.89,  55.  ,  24.73,  23.66,  63.  ],\n",
              "       [148.  ,   8.2 ,   7.75,  34.46,  46.  ,  26.53,   6.04,  64.  ],\n",
              "       [154.  ,   4.5 ,   4.75,  23.52,  43.  ,  25.76,   0.  ,  53.  ]],\n",
              "      dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DuP6ehxtNb32",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 884
        },
        "outputId": "e60b6f1f-a6a0-4404-9ba2-2bcf1833a830"
      },
      "source": [
        "numeric_layer = tf.keras.layers.DenseFeatures(numeric_columns)\n",
        "numeric_layer(train_batch).numpy()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.852, -0.817,  0.017,  0.35 ,  0.262, -0.113,  2.388,  1.097],\n",
              "       [ 0.233, -0.755, -1.47 , -0.574,  0.363, -0.591, -0.568, -0.708],\n",
              "       [-1.005, -0.817, -1.178, -1.735, -0.441, -1.82 , -0.677, -1.818],\n",
              "       [-0.902, -0.817, -0.541, -1.721, -0.24 , -1.332, -0.667, -1.61 ],\n",
              "       [-1.005, -0.483,  0.298,  0.054,  1.066,  0.588, -0.529, -0.985],\n",
              "       [-1.315, -0.496, -1.02 , -0.327,  0.564, -0.316, -0.687,  0.611],\n",
              "       [-0.799, -0.594,  0.545,  1.224,  1.568,  1.214, -0.04 , -0.222],\n",
              "       [-1.108,  0.136,  1.108, -0.71 ,  1.468, -0.672, -0.687,  0.611],\n",
              "       [-1.211, -0.55 , -0.393, -1.374, -0.441, -0.558,  0.361, -1.194],\n",
              "       [ 0.284,  0.305,  0.041, -0.24 ,  0.463, -0.345,  0.072, -0.083],\n",
              "       [-0.077, -0.514, -0.798, -1.364,  0.262, -0.276, -0.391, -1.332],\n",
              "       [ 1.368,  2.082,  0.733,  0.516, -0.642, -0.804,  0.815,  0.75 ],\n",
              "       [-0.077, -0.817, -1.233, -0.951,  0.162, -0.842, -0.687, -1.818],\n",
              "       [-0.18 , -0.483, -0.517, -0.509, -1.244, -0.342, -0.236, -0.916],\n",
              "       [ 0.233, -0.283, -1.099, -0.201,  0.062, -0.013,  1.713, -0.43 ],\n",
              "       [ 1.265, -0.167, -0.566,  0.77 ,  0.865,  1.289,  0.064, -0.083],\n",
              "       [-0.18 ,  1.143,  1.301,  0.184, -1.847,  0.78 ,  0.511,  1.166],\n",
              "       [-1.521, -0.483, -0.22 , -0.057,  1.267, -0.913,  0.189,  1.235],\n",
              "       [-0.18 ,  0.982, -1.593, -1.034,  0.262, -0.828,  2.017, -0.847],\n",
              "       [ 0.439, -0.817,  0.911,  0.034,  0.664,  0.456, -0.353,  1.374],\n",
              "       [ 3.637,  0.831,  1.301,  0.861, -0.34 ,  0.349, -0.369,  0.958],\n",
              "       [ 4.153,  1.677, -0.991,  0.7  , -1.546, -0.305,  3.003,  0.333],\n",
              "       [ 1.471, -0.683, -1.163,  1.122, -0.039,  0.193,  1.504,  1.166],\n",
              "       [-1.005, -0.817, -0.215,  0.612, -0.139,  1.429, -0.529,  0.194],\n",
              "       [-1.108, -0.149, -0.852,  0.637, -1.244, -0.596, -0.652,  0.056],\n",
              "       [-0.18 , -0.817,  0.555,  0.706, -0.441,  0.714, -0.687,  0.819],\n",
              "       [-0.18 , -0.813, -0.976, -0.86 , -0.843, -0.314, -0.687, -1.818],\n",
              "       [-0.489, -0.817, -0.887, -1.675,  1.167, -1.28 , -0.604, -1.679],\n",
              "       [-0.386,  3.191, -0.319,  0.261,  0.062,  0.307, -0.687,  0.541],\n",
              "       [-0.489, -0.706, -0.531, -1.646,  1.267, -1.16 ,  0.235, -1.055],\n",
              "       [-0.386, -0.236, -1.015, -0.318, -0.24 ,  0.034, -0.145,  0.541],\n",
              "       [-0.18 , -0.372, -0.551, -1.401, -0.139, -1.212, -0.604, -0.43 ],\n",
              "       [-0.18 ,  2.211, -0.63 ,  0.307,  0.664, -0.037,  1.64 ,  0.403],\n",
              "       [-0.696,  0.074,  3.775,  0.765,  0.062, -0.691, -0.604, -0.083],\n",
              "       [-0.077,  0.822, -1.277,  0.35 ,  0.765, -0.271,  1.817,  0.75 ],\n",
              "       [-0.902,  0.853,  5.212, -0.447,  0.664, -0.198,  0.712,  0.403],\n",
              "       [-0.18 ,  1.41 , -0.487,  1.212, -1.144,  0.517,  0.481,  0.611],\n",
              "       [ 0.13 , -0.817,  0.15 ,  0.248, -1.244,  0.399, -0.637, -0.361],\n",
              "       [-1.108, -0.216, -0.536, -1.554,  0.162, -1.188,  0.064, -0.777],\n",
              "       [-1.211, -0.817,  1.597, -0.494,  1.267, -0.151, -0.586, -1.887],\n",
              "       [-0.696,  0.118, -0.907,  0.282, -0.34 ,  0.986,  2.764, -0.916],\n",
              "       [-0.696, -0.585, -0.956, -1.175, -0.742, -1.415, -0.687,  1.235],\n",
              "       [ 2.606, -0.817,  0.343,  0.915,  1.769,  0.674, -0.387,  0.472],\n",
              "       [ 3.534,  0.519, -0.902,  0.893,  1.87 ,  0.157,  1.588,  1.166],\n",
              "       [-0.696, -0.817,  0.007,  1.211, -0.441, -0.013, -0.312,  0.264],\n",
              "       [-0.283,  0.519,  0.59 ,  0.039,  1.267, -0.466,  5.209, -0.153],\n",
              "       [ 0.233,  3.236, -0.215, -0.137,  0.765,  0.01 , -0.687,  0.472],\n",
              "       [-0.18 ,  0.252,  0.891,  0.582,  0.162, -0.335,  0.273,  1.374],\n",
              "       [ 0.542,  1.009,  1.469,  1.178, -0.742,  0.091, -0.442,  1.444],\n",
              "       [ 0.852,  0.185, -0.013, -0.249, -1.043, -0.091, -0.687,  0.68 ]],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ylwxZUHWNbzk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CATEGORIES = {\n",
        "    'famhist': ['Absent', 'Present']\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "soi2VyvtNbvg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "categorical_columns = []\n",
        "for feature, vocab in CATEGORIES.items():\n",
        "  cat_col = tf.feature_column.categorical_column_with_vocabulary_list(\n",
        "        key=feature, vocabulary_list=vocab)\n",
        "  categorical_columns.append(tf.feature_column.indicator_column(cat_col))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2oQt1sjNbWb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b9cf60d1-91c5-455a-a2ba-c9f7cfbc4fc8"
      },
      "source": [
        "# See what you just created.\n",
        "categorical_columns"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[IndicatorColumn(categorical_column=VocabularyListCategoricalColumn(key='famhist', vocabulary_list=('Absent', 'Present'), dtype=tf.string, default_value=-1, num_oov_buckets=0))]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YLc71ebNYsW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "804417b4-3e27-439b-8832-4aac2e2063d0"
      },
      "source": [
        "categorical_layer = tf.keras.layers.DenseFeatures(categorical_columns)\n",
        "print(categorical_layer(train_batch).numpy()[0])"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-2.0.0/python3.6/tensorflow_core/python/feature_column/feature_column_v2.py:4276: IndicatorColumn._variable_shape (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
            "WARNING:tensorflow:From /tensorflow-2.0.0/python3.6/tensorflow_core/python/feature_column/feature_column_v2.py:4331: VocabularyListCategoricalColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
            "[0. 1.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLScVBP0NY3q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Combined preprocessing layer\n",
        "#Add the two feature column collections and pass them to a tf.keras.layers.DenseFeatures to create an input layer that will extract and preprocess both input types:\n",
        "\n",
        "preprocessing_layer = tf.keras.layers.DenseFeatures(categorical_columns+numeric_columns)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rI80GcK7NY_b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4d4f9756-4cec-45fe-e49c-a9aff88bdec7"
      },
      "source": [
        "print(preprocessing_layer(train_batch).numpy()[0])"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0.     1.     0.852 -0.817  0.017  0.35   0.262 -0.113  2.388  1.097]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_niGrnUxNZFy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# build the model\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "  preprocessing_layer,\n",
        "  tf.keras.layers.Dense(128, kernel_regularizer=tf.keras.regularizers.l2(0.01), activation='relu'),\n",
        "  tf.keras.layers.Dropout(0.2),\n",
        "  tf.keras.layers.Dense(128, kernel_regularizer=tf.keras.regularizers.l2(0.01), activation='relu'),\n",
        "  tf.keras.layers.Dropout(0.2),\n",
        "  tf.keras.layers.Dense(128, kernel_regularizer=tf.keras.regularizers.l2(0.01), activation='relu'),\n",
        "  tf.keras.layers.Dropout(0.2),\n",
        "\n",
        "  tf.keras.layers.Dense(1, activation='sigmoid'),\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    loss='binary_crossentropy',\n",
        "    optimizer='adamax',\n",
        "    metrics=['accuracy'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9T32YQkNZJd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = packed_train_data.shuffle(600)\n",
        "test_data = packed_test_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "erEqc1GRNZMw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "268a619e-70d4-4a6b-ea49-300a670f14b7"
      },
      "source": [
        "model.fit(train_data, epochs=400)"
      ],
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/400\n",
            "8/8 [==============================] - 1s 74ms/step - loss: 0.6484 - accuracy: 0.6407\n",
            "Epoch 2/400\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.6055 - accuracy: 0.6910\n",
            "Epoch 3/400\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5687 - accuracy: 0.7060\n",
            "Epoch 4/400\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.5421 - accuracy: 0.7211\n",
            "Epoch 5/400\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.5332 - accuracy: 0.7337\n",
            "Epoch 6/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5310 - accuracy: 0.7337\n",
            "Epoch 7/400\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.5286 - accuracy: 0.7412\n",
            "Epoch 8/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5226 - accuracy: 0.7337\n",
            "Epoch 9/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5128 - accuracy: 0.7462\n",
            "Epoch 10/400\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.4886 - accuracy: 0.7513\n",
            "Epoch 11/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5093 - accuracy: 0.7487\n",
            "Epoch 12/400\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.4867 - accuracy: 0.7487\n",
            "Epoch 13/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.4927 - accuracy: 0.7387\n",
            "Epoch 14/400\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5070 - accuracy: 0.7412\n",
            "Epoch 15/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.4925 - accuracy: 0.7588\n",
            "Epoch 16/400\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.4872 - accuracy: 0.7563\n",
            "Epoch 17/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.4879 - accuracy: 0.7663\n",
            "Epoch 18/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.4900 - accuracy: 0.7563\n",
            "Epoch 19/400\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.4878 - accuracy: 0.7714\n",
            "Epoch 20/400\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.4871 - accuracy: 0.7688\n",
            "Epoch 21/400\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.4990 - accuracy: 0.7487\n",
            "Epoch 22/400\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.4809 - accuracy: 0.7638\n",
            "Epoch 23/400\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.4632 - accuracy: 0.7663\n",
            "Epoch 24/400\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.4561 - accuracy: 0.7714\n",
            "Epoch 25/400\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.4842 - accuracy: 0.7688\n",
            "Epoch 26/400\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.4778 - accuracy: 0.7663\n",
            "Epoch 27/400\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.4724 - accuracy: 0.7814\n",
            "Epoch 28/400\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.4682 - accuracy: 0.7739\n",
            "Epoch 29/400\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.4699 - accuracy: 0.7688\n",
            "Epoch 30/400\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.4466 - accuracy: 0.7864\n",
            "Epoch 31/400\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.4679 - accuracy: 0.7714\n",
            "Epoch 32/400\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.4640 - accuracy: 0.7839\n",
            "Epoch 33/400\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.4651 - accuracy: 0.7814\n",
            "Epoch 34/400\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.4805 - accuracy: 0.7814\n",
            "Epoch 35/400\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.4603 - accuracy: 0.7915\n",
            "Epoch 36/400\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.4549 - accuracy: 0.8015\n",
            "Epoch 37/400\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.4436 - accuracy: 0.7739\n",
            "Epoch 38/400\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.4396 - accuracy: 0.7915\n",
            "Epoch 39/400\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.4533 - accuracy: 0.8040\n",
            "Epoch 40/400\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.4444 - accuracy: 0.7889\n",
            "Epoch 41/400\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.4298 - accuracy: 0.7864\n",
            "Epoch 42/400\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.4377 - accuracy: 0.7889\n",
            "Epoch 43/400\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.4333 - accuracy: 0.7990\n",
            "Epoch 44/400\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.4451 - accuracy: 0.7814\n",
            "Epoch 45/400\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.4503 - accuracy: 0.7915\n",
            "Epoch 46/400\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.4583 - accuracy: 0.7915\n",
            "Epoch 47/400\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.4619 - accuracy: 0.7789\n",
            "Epoch 48/400\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.4271 - accuracy: 0.8015\n",
            "Epoch 49/400\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.4614 - accuracy: 0.8090\n",
            "Epoch 50/400\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.4455 - accuracy: 0.8040\n",
            "Epoch 51/400\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.4350 - accuracy: 0.8040\n",
            "Epoch 52/400\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.4450 - accuracy: 0.8015\n",
            "Epoch 53/400\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.4262 - accuracy: 0.8040\n",
            "Epoch 54/400\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.4330 - accuracy: 0.8090\n",
            "Epoch 55/400\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.4287 - accuracy: 0.8015\n",
            "Epoch 56/400\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.4279 - accuracy: 0.7915\n",
            "Epoch 57/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.4344 - accuracy: 0.8116\n",
            "Epoch 58/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.4078 - accuracy: 0.8040\n",
            "Epoch 59/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.4358 - accuracy: 0.8141\n",
            "Epoch 60/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.4338 - accuracy: 0.8090\n",
            "Epoch 61/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.4214 - accuracy: 0.8090\n",
            "Epoch 62/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.4257 - accuracy: 0.8015\n",
            "Epoch 63/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.4345 - accuracy: 0.8216\n",
            "Epoch 64/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.4186 - accuracy: 0.8090\n",
            "Epoch 65/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.4104 - accuracy: 0.8291\n",
            "Epoch 66/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.4209 - accuracy: 0.8040\n",
            "Epoch 67/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.3943 - accuracy: 0.8116\n",
            "Epoch 68/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.4209 - accuracy: 0.8191\n",
            "Epoch 69/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.4266 - accuracy: 0.8191\n",
            "Epoch 70/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.4141 - accuracy: 0.8141\n",
            "Epoch 71/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.3973 - accuracy: 0.8216\n",
            "Epoch 72/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.3907 - accuracy: 0.8166\n",
            "Epoch 73/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.4205 - accuracy: 0.8141\n",
            "Epoch 74/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.3866 - accuracy: 0.8216\n",
            "Epoch 75/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.4018 - accuracy: 0.8166\n",
            "Epoch 76/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.4047 - accuracy: 0.8291\n",
            "Epoch 77/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.4047 - accuracy: 0.8291\n",
            "Epoch 78/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.3776 - accuracy: 0.8116\n",
            "Epoch 79/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.4109 - accuracy: 0.8141\n",
            "Epoch 80/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.3853 - accuracy: 0.8191\n",
            "Epoch 81/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.3840 - accuracy: 0.8116\n",
            "Epoch 82/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.3973 - accuracy: 0.8392\n",
            "Epoch 83/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.3891 - accuracy: 0.8342\n",
            "Epoch 84/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.4033 - accuracy: 0.8266\n",
            "Epoch 85/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.4013 - accuracy: 0.8241\n",
            "Epoch 86/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.3709 - accuracy: 0.8241\n",
            "Epoch 87/400\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.3955 - accuracy: 0.8166\n",
            "Epoch 88/400\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.3894 - accuracy: 0.8317\n",
            "Epoch 89/400\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.3772 - accuracy: 0.8367\n",
            "Epoch 90/400\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.3865 - accuracy: 0.8367\n",
            "Epoch 91/400\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.3844 - accuracy: 0.8317\n",
            "Epoch 92/400\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.3878 - accuracy: 0.8317\n",
            "Epoch 93/400\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.3901 - accuracy: 0.8166\n",
            "Epoch 94/400\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.3831 - accuracy: 0.8417\n",
            "Epoch 95/400\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.3708 - accuracy: 0.8442\n",
            "Epoch 96/400\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.3810 - accuracy: 0.8367\n",
            "Epoch 97/400\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.3717 - accuracy: 0.8291\n",
            "Epoch 98/400\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.3734 - accuracy: 0.8442\n",
            "Epoch 99/400\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.3790 - accuracy: 0.8442\n",
            "Epoch 100/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.3782 - accuracy: 0.8342\n",
            "Epoch 101/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.3637 - accuracy: 0.8442\n",
            "Epoch 102/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.3797 - accuracy: 0.8442\n",
            "Epoch 103/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.3782 - accuracy: 0.8442\n",
            "Epoch 104/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.3718 - accuracy: 0.8467\n",
            "Epoch 105/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.3602 - accuracy: 0.8543\n",
            "Epoch 106/400\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.3545 - accuracy: 0.8543\n",
            "Epoch 107/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.3722 - accuracy: 0.8518\n",
            "Epoch 108/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.3538 - accuracy: 0.8618\n",
            "Epoch 109/400\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.3500 - accuracy: 0.8543\n",
            "Epoch 110/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.3569 - accuracy: 0.8417\n",
            "Epoch 111/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.3590 - accuracy: 0.8543\n",
            "Epoch 112/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.3517 - accuracy: 0.8543\n",
            "Epoch 113/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.3515 - accuracy: 0.8543\n",
            "Epoch 114/400\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.3477 - accuracy: 0.8593\n",
            "Epoch 115/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.3571 - accuracy: 0.8492\n",
            "Epoch 116/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.3460 - accuracy: 0.8568\n",
            "Epoch 117/400\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.3342 - accuracy: 0.8568\n",
            "Epoch 118/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.3438 - accuracy: 0.8744\n",
            "Epoch 119/400\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.3367 - accuracy: 0.8518\n",
            "Epoch 120/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.3524 - accuracy: 0.8568\n",
            "Epoch 121/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.3444 - accuracy: 0.8543\n",
            "Epoch 122/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.3423 - accuracy: 0.8668\n",
            "Epoch 123/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.3402 - accuracy: 0.8643\n",
            "Epoch 124/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.3391 - accuracy: 0.8593\n",
            "Epoch 125/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.3641 - accuracy: 0.8467\n",
            "Epoch 126/400\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.3329 - accuracy: 0.8618\n",
            "Epoch 127/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.3322 - accuracy: 0.8794\n",
            "Epoch 128/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.3384 - accuracy: 0.8668\n",
            "Epoch 129/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.3448 - accuracy: 0.8618\n",
            "Epoch 130/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.3428 - accuracy: 0.8693\n",
            "Epoch 131/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.3378 - accuracy: 0.8769\n",
            "Epoch 132/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.3266 - accuracy: 0.8668\n",
            "Epoch 133/400\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.3160 - accuracy: 0.8769\n",
            "Epoch 134/400\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.3195 - accuracy: 0.8769\n",
            "Epoch 135/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.3211 - accuracy: 0.8794\n",
            "Epoch 136/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.3225 - accuracy: 0.8693\n",
            "Epoch 137/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.3150 - accuracy: 0.8693\n",
            "Epoch 138/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.3131 - accuracy: 0.8794\n",
            "Epoch 139/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.3190 - accuracy: 0.8668\n",
            "Epoch 140/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.3096 - accuracy: 0.8819\n",
            "Epoch 141/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.3182 - accuracy: 0.8769\n",
            "Epoch 142/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.3376 - accuracy: 0.8794\n",
            "Epoch 143/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.3054 - accuracy: 0.8894\n",
            "Epoch 144/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.3133 - accuracy: 0.8769\n",
            "Epoch 145/400\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.3118 - accuracy: 0.8844\n",
            "Epoch 146/400\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.3029 - accuracy: 0.8995\n",
            "Epoch 147/400\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.3028 - accuracy: 0.8719\n",
            "Epoch 148/400\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.3034 - accuracy: 0.8769\n",
            "Epoch 149/400\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.3120 - accuracy: 0.8819\n",
            "Epoch 150/400\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.3010 - accuracy: 0.8744\n",
            "Epoch 151/400\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.3026 - accuracy: 0.8894\n",
            "Epoch 152/400\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.3145 - accuracy: 0.8744\n",
            "Epoch 153/400\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.3075 - accuracy: 0.8920\n",
            "Epoch 154/400\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.3045 - accuracy: 0.8894\n",
            "Epoch 155/400\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.2848 - accuracy: 0.8920\n",
            "Epoch 156/400\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.2842 - accuracy: 0.9020\n",
            "Epoch 157/400\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.3084 - accuracy: 0.8869\n",
            "Epoch 158/400\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.2919 - accuracy: 0.8894\n",
            "Epoch 159/400\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.2939 - accuracy: 0.8869\n",
            "Epoch 160/400\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.3054 - accuracy: 0.8894\n",
            "Epoch 161/400\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.2818 - accuracy: 0.8920\n",
            "Epoch 162/400\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.2926 - accuracy: 0.8970\n",
            "Epoch 163/400\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.2969 - accuracy: 0.8920\n",
            "Epoch 164/400\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.2931 - accuracy: 0.8920\n",
            "Epoch 165/400\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.2826 - accuracy: 0.8945\n",
            "Epoch 166/400\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.2826 - accuracy: 0.8894\n",
            "Epoch 167/400\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.2952 - accuracy: 0.8844\n",
            "Epoch 168/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2718 - accuracy: 0.8945\n",
            "Epoch 169/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2856 - accuracy: 0.8970\n",
            "Epoch 170/400\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.2739 - accuracy: 0.8945\n",
            "Epoch 171/400\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.2858 - accuracy: 0.8794\n",
            "Epoch 172/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2773 - accuracy: 0.8920\n",
            "Epoch 173/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2810 - accuracy: 0.8995\n",
            "Epoch 174/400\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.2625 - accuracy: 0.9020\n",
            "Epoch 175/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2631 - accuracy: 0.9070\n",
            "Epoch 176/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2806 - accuracy: 0.8970\n",
            "Epoch 177/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2620 - accuracy: 0.8970\n",
            "Epoch 178/400\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.2622 - accuracy: 0.9121\n",
            "Epoch 179/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2668 - accuracy: 0.8995\n",
            "Epoch 180/400\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.2697 - accuracy: 0.9121\n",
            "Epoch 181/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2644 - accuracy: 0.9070\n",
            "Epoch 182/400\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.2722 - accuracy: 0.9070\n",
            "Epoch 183/400\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.2576 - accuracy: 0.9121\n",
            "Epoch 184/400\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.2520 - accuracy: 0.9095\n",
            "Epoch 185/400\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.2551 - accuracy: 0.9020\n",
            "Epoch 186/400\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.2630 - accuracy: 0.9121\n",
            "Epoch 187/400\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.2630 - accuracy: 0.9095\n",
            "Epoch 188/400\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.2431 - accuracy: 0.9070\n",
            "Epoch 189/400\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.2569 - accuracy: 0.9121\n",
            "Epoch 190/400\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.2584 - accuracy: 0.9121\n",
            "Epoch 191/400\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.2525 - accuracy: 0.9070\n",
            "Epoch 192/400\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.2401 - accuracy: 0.9121\n",
            "Epoch 193/400\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.2544 - accuracy: 0.9121\n",
            "Epoch 194/400\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.2416 - accuracy: 0.9121\n",
            "Epoch 195/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2521 - accuracy: 0.9020\n",
            "Epoch 196/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2516 - accuracy: 0.9171\n",
            "Epoch 197/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2379 - accuracy: 0.9171\n",
            "Epoch 198/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2329 - accuracy: 0.9221\n",
            "Epoch 199/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2366 - accuracy: 0.9070\n",
            "Epoch 200/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2369 - accuracy: 0.9146\n",
            "Epoch 201/400\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.2226 - accuracy: 0.9146\n",
            "Epoch 202/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2512 - accuracy: 0.9121\n",
            "Epoch 203/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2357 - accuracy: 0.9171\n",
            "Epoch 204/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2447 - accuracy: 0.9271\n",
            "Epoch 205/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2423 - accuracy: 0.9271\n",
            "Epoch 206/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2274 - accuracy: 0.9171\n",
            "Epoch 207/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2261 - accuracy: 0.9221\n",
            "Epoch 208/400\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.2217 - accuracy: 0.9171\n",
            "Epoch 209/400\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.2353 - accuracy: 0.9322\n",
            "Epoch 210/400\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.2275 - accuracy: 0.9171\n",
            "Epoch 211/400\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.2229 - accuracy: 0.9271\n",
            "Epoch 212/400\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.2177 - accuracy: 0.9246\n",
            "Epoch 213/400\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.2165 - accuracy: 0.9422\n",
            "Epoch 214/400\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.2237 - accuracy: 0.9146\n",
            "Epoch 215/400\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.2194 - accuracy: 0.9322\n",
            "Epoch 216/400\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.2092 - accuracy: 0.9221\n",
            "Epoch 217/400\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.2235 - accuracy: 0.9171\n",
            "Epoch 218/400\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.2242 - accuracy: 0.9296\n",
            "Epoch 219/400\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.2260 - accuracy: 0.9246\n",
            "Epoch 220/400\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.2213 - accuracy: 0.9221\n",
            "Epoch 221/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2194 - accuracy: 0.9322\n",
            "Epoch 222/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2268 - accuracy: 0.9296\n",
            "Epoch 223/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2184 - accuracy: 0.9347\n",
            "Epoch 224/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2035 - accuracy: 0.9472\n",
            "Epoch 225/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1986 - accuracy: 0.9397\n",
            "Epoch 226/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2228 - accuracy: 0.9271\n",
            "Epoch 227/400\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.2033 - accuracy: 0.9372\n",
            "Epoch 228/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2076 - accuracy: 0.9296\n",
            "Epoch 229/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1937 - accuracy: 0.9447\n",
            "Epoch 230/400\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1958 - accuracy: 0.9497\n",
            "Epoch 231/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1953 - accuracy: 0.9372\n",
            "Epoch 232/400\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1865 - accuracy: 0.9422\n",
            "Epoch 233/400\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.2039 - accuracy: 0.9397\n",
            "Epoch 234/400\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.2034 - accuracy: 0.9422\n",
            "Epoch 235/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1985 - accuracy: 0.9472\n",
            "Epoch 236/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2043 - accuracy: 0.9447\n",
            "Epoch 237/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1963 - accuracy: 0.9397\n",
            "Epoch 238/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2010 - accuracy: 0.9221\n",
            "Epoch 239/400\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1975 - accuracy: 0.9322\n",
            "Epoch 240/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1958 - accuracy: 0.9447\n",
            "Epoch 241/400\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1884 - accuracy: 0.9397\n",
            "Epoch 242/400\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1987 - accuracy: 0.9397\n",
            "Epoch 243/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1951 - accuracy: 0.9472\n",
            "Epoch 244/400\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1855 - accuracy: 0.9447\n",
            "Epoch 245/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1910 - accuracy: 0.9397\n",
            "Epoch 246/400\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1753 - accuracy: 0.9447\n",
            "Epoch 247/400\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.1878 - accuracy: 0.9397\n",
            "Epoch 248/400\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1841 - accuracy: 0.9472\n",
            "Epoch 249/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1871 - accuracy: 0.9523\n",
            "Epoch 250/400\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.1843 - accuracy: 0.9497\n",
            "Epoch 251/400\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.1823 - accuracy: 0.9472\n",
            "Epoch 252/400\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.1731 - accuracy: 0.9523\n",
            "Epoch 253/400\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.1820 - accuracy: 0.9372\n",
            "Epoch 254/400\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.1645 - accuracy: 0.9598\n",
            "Epoch 255/400\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.1838 - accuracy: 0.9447\n",
            "Epoch 256/400\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.1569 - accuracy: 0.9598\n",
            "Epoch 257/400\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.1776 - accuracy: 0.9648\n",
            "Epoch 258/400\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.1773 - accuracy: 0.9472\n",
            "Epoch 259/400\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.1711 - accuracy: 0.9548\n",
            "Epoch 260/400\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.1826 - accuracy: 0.9548\n",
            "Epoch 261/400\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.1746 - accuracy: 0.9472\n",
            "Epoch 262/400\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1684 - accuracy: 0.9447\n",
            "Epoch 263/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1713 - accuracy: 0.9523\n",
            "Epoch 264/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1662 - accuracy: 0.9472\n",
            "Epoch 265/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1794 - accuracy: 0.9447\n",
            "Epoch 266/400\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1754 - accuracy: 0.9548\n",
            "Epoch 267/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1507 - accuracy: 0.9497\n",
            "Epoch 268/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1623 - accuracy: 0.9598\n",
            "Epoch 269/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1621 - accuracy: 0.9523\n",
            "Epoch 270/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1535 - accuracy: 0.9623\n",
            "Epoch 271/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1602 - accuracy: 0.9598\n",
            "Epoch 272/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1598 - accuracy: 0.9623\n",
            "Epoch 273/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1550 - accuracy: 0.9648\n",
            "Epoch 274/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1486 - accuracy: 0.9523\n",
            "Epoch 275/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1722 - accuracy: 0.9447\n",
            "Epoch 276/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1588 - accuracy: 0.9598\n",
            "Epoch 277/400\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.1580 - accuracy: 0.9623\n",
            "Epoch 278/400\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1522 - accuracy: 0.9548\n",
            "Epoch 279/400\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1574 - accuracy: 0.9573\n",
            "Epoch 280/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1577 - accuracy: 0.9472\n",
            "Epoch 281/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1505 - accuracy: 0.9698\n",
            "Epoch 282/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1608 - accuracy: 0.9523\n",
            "Epoch 283/400\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1478 - accuracy: 0.9673\n",
            "Epoch 284/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1529 - accuracy: 0.9673\n",
            "Epoch 285/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1428 - accuracy: 0.9648\n",
            "Epoch 286/400\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1448 - accuracy: 0.9648\n",
            "Epoch 287/400\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.1460 - accuracy: 0.9673\n",
            "Epoch 288/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1422 - accuracy: 0.9724\n",
            "Epoch 289/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1526 - accuracy: 0.9648\n",
            "Epoch 290/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1409 - accuracy: 0.9724\n",
            "Epoch 291/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1418 - accuracy: 0.9648\n",
            "Epoch 292/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1457 - accuracy: 0.9698\n",
            "Epoch 293/400\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1549 - accuracy: 0.9573\n",
            "Epoch 294/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1204 - accuracy: 0.9698\n",
            "Epoch 295/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1429 - accuracy: 0.9648\n",
            "Epoch 296/400\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1459 - accuracy: 0.9648\n",
            "Epoch 297/400\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.1380 - accuracy: 0.9824\n",
            "Epoch 298/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1343 - accuracy: 0.9799\n",
            "Epoch 299/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1362 - accuracy: 0.9698\n",
            "Epoch 300/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1306 - accuracy: 0.9724\n",
            "Epoch 301/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1335 - accuracy: 0.9774\n",
            "Epoch 302/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1271 - accuracy: 0.9799\n",
            "Epoch 303/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1327 - accuracy: 0.9698\n",
            "Epoch 304/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1229 - accuracy: 0.9749\n",
            "Epoch 305/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1352 - accuracy: 0.9673\n",
            "Epoch 306/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1331 - accuracy: 0.9623\n",
            "Epoch 307/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1239 - accuracy: 0.9749\n",
            "Epoch 308/400\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1277 - accuracy: 0.9724\n",
            "Epoch 309/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1280 - accuracy: 0.9749\n",
            "Epoch 310/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1235 - accuracy: 0.9774\n",
            "Epoch 311/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1173 - accuracy: 0.9673\n",
            "Epoch 312/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1292 - accuracy: 0.9724\n",
            "Epoch 313/400\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1240 - accuracy: 0.9749\n",
            "Epoch 314/400\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.1182 - accuracy: 0.9749\n",
            "Epoch 315/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1275 - accuracy: 0.9774\n",
            "Epoch 316/400\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1315 - accuracy: 0.9724\n",
            "Epoch 317/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1197 - accuracy: 0.9648\n",
            "Epoch 318/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1200 - accuracy: 0.9698\n",
            "Epoch 319/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1140 - accuracy: 0.9749\n",
            "Epoch 320/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1306 - accuracy: 0.9724\n",
            "Epoch 321/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1210 - accuracy: 0.9724\n",
            "Epoch 322/400\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1169 - accuracy: 0.9749\n",
            "Epoch 323/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1158 - accuracy: 0.9824\n",
            "Epoch 324/400\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1138 - accuracy: 0.9749\n",
            "Epoch 325/400\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1138 - accuracy: 0.9824\n",
            "Epoch 326/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1128 - accuracy: 0.9799\n",
            "Epoch 327/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1148 - accuracy: 0.9799\n",
            "Epoch 328/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1047 - accuracy: 0.9799\n",
            "Epoch 329/400\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1087 - accuracy: 0.9899\n",
            "Epoch 330/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1061 - accuracy: 0.9925\n",
            "Epoch 331/400\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1178 - accuracy: 0.9799\n",
            "Epoch 332/400\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1090 - accuracy: 0.9774\n",
            "Epoch 333/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1053 - accuracy: 0.9849\n",
            "Epoch 334/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1050 - accuracy: 0.9774\n",
            "Epoch 335/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1122 - accuracy: 0.9774\n",
            "Epoch 336/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1060 - accuracy: 0.9824\n",
            "Epoch 337/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0953 - accuracy: 0.9799\n",
            "Epoch 338/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1087 - accuracy: 0.9874\n",
            "Epoch 339/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0990 - accuracy: 0.9824\n",
            "Epoch 340/400\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0978 - accuracy: 0.9874\n",
            "Epoch 341/400\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1085 - accuracy: 0.9824\n",
            "Epoch 342/400\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1066 - accuracy: 0.9874\n",
            "Epoch 343/400\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0931 - accuracy: 0.9874\n",
            "Epoch 344/400\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0971 - accuracy: 0.9849\n",
            "Epoch 345/400\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1009 - accuracy: 0.9925\n",
            "Epoch 346/400\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.1042 - accuracy: 0.9824\n",
            "Epoch 347/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0982 - accuracy: 0.9799\n",
            "Epoch 348/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1061 - accuracy: 0.9824\n",
            "Epoch 349/400\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1016 - accuracy: 0.9824\n",
            "Epoch 350/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0902 - accuracy: 0.9849\n",
            "Epoch 351/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0929 - accuracy: 0.9899\n",
            "Epoch 352/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0943 - accuracy: 0.9874\n",
            "Epoch 353/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1109 - accuracy: 0.9749\n",
            "Epoch 354/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1016 - accuracy: 0.9799\n",
            "Epoch 355/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0936 - accuracy: 0.9950\n",
            "Epoch 356/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0933 - accuracy: 0.9899\n",
            "Epoch 357/400\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0897 - accuracy: 0.9899\n",
            "Epoch 358/400\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0808 - accuracy: 0.9975\n",
            "Epoch 359/400\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0862 - accuracy: 0.9774\n",
            "Epoch 360/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0874 - accuracy: 0.9874\n",
            "Epoch 361/400\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0812 - accuracy: 0.9899\n",
            "Epoch 362/400\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0984 - accuracy: 0.9799\n",
            "Epoch 363/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0993 - accuracy: 0.9874\n",
            "Epoch 364/400\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0903 - accuracy: 0.9899\n",
            "Epoch 365/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0908 - accuracy: 0.9899\n",
            "Epoch 366/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0911 - accuracy: 0.9950\n",
            "Epoch 367/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0798 - accuracy: 0.9874\n",
            "Epoch 368/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0834 - accuracy: 0.9899\n",
            "Epoch 369/400\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0964 - accuracy: 0.9849\n",
            "Epoch 370/400\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0904 - accuracy: 0.9899\n",
            "Epoch 371/400\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0797 - accuracy: 0.9950\n",
            "Epoch 372/400\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0815 - accuracy: 0.9950\n",
            "Epoch 373/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0890 - accuracy: 0.9849\n",
            "Epoch 374/400\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0736 - accuracy: 0.9950\n",
            "Epoch 375/400\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0875 - accuracy: 0.9899\n",
            "Epoch 376/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0782 - accuracy: 0.9975\n",
            "Epoch 377/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0818 - accuracy: 0.9925\n",
            "Epoch 378/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0774 - accuracy: 0.9899\n",
            "Epoch 379/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0807 - accuracy: 0.9925\n",
            "Epoch 380/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0724 - accuracy: 0.9950\n",
            "Epoch 381/400\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0801 - accuracy: 0.9950\n",
            "Epoch 382/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0782 - accuracy: 0.9975\n",
            "Epoch 383/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0740 - accuracy: 0.9899\n",
            "Epoch 384/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0772 - accuracy: 0.9975\n",
            "Epoch 385/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0829 - accuracy: 0.9874\n",
            "Epoch 386/400\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0811 - accuracy: 0.9849\n",
            "Epoch 387/400\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0717 - accuracy: 0.9925\n",
            "Epoch 388/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0821 - accuracy: 0.9849\n",
            "Epoch 389/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0673 - accuracy: 0.9975\n",
            "Epoch 390/400\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0768 - accuracy: 0.9975\n",
            "Epoch 391/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0794 - accuracy: 0.9849\n",
            "Epoch 392/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0714 - accuracy: 0.9925\n",
            "Epoch 393/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0743 - accuracy: 0.9925\n",
            "Epoch 394/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0649 - accuracy: 0.9975\n",
            "Epoch 395/400\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0721 - accuracy: 0.9950\n",
            "Epoch 396/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0720 - accuracy: 0.9899\n",
            "Epoch 397/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0787 - accuracy: 0.9899\n",
            "Epoch 398/400\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0696 - accuracy: 0.9899\n",
            "Epoch 399/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0697 - accuracy: 0.9950\n",
            "Epoch 400/400\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0605 - accuracy: 0.9975\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f884721def0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 155
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FlKP62rRWe9E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "d201666f-4ab5-4e4d-a2d6-dbd793e7d957"
      },
      "source": [
        "train_loss, train_accuracy = model.evaluate(train_data)\n",
        "\n",
        "print('\\n\\nTrain Loss {:.4f}, Train Accuracy {:.2%}'.format(train_loss, train_accuracy))"
      ],
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1436 - accuracy: 0.9698\n",
            "\n",
            "\n",
            "Train Loss 0.1436, Train Accuracy 96.98%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWwgAf_ZNZQ_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "d32faddb-1803-41bb-d34f-20d3dfa41ca9"
      },
      "source": [
        "test_loss, test_accuracy = model.evaluate(test_data)\n",
        "\n",
        "print('\\n\\nTest Loss {:.4f}, Test Accuracy {:.2%}'.format(test_loss, test_accuracy))"
      ],
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2/2 [==============================] - 0s 73ms/step - loss: 0.5474 - accuracy: 0.7969\n",
            "\n",
            "\n",
            "Test Loss 0.5474, Test Accuracy 79.69%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ev89wYStNZWF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "80f6293c-d5e6-4f75-d2c0-19084805296b"
      },
      "source": [
        "predictions = model.predict(test_data)\n",
        "\n",
        "# Show some results\n",
        "for prediction, CHD in zip(predictions[:10], list(test_data)[0][1][:10]):\n",
        "  print(\"Predicted diagnosis certainty: {:.2%}\".format(prediction[0]),\n",
        "        \" | Actual outcome: \",\n",
        "        (\"Patient has CHD\" if bool(CHD) else \"Patient doesn't have CHD\"))\n"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicted diagnosis certainty: 0.00%  | Actual outcome:  Patient doesn't have CHD\n",
            "Predicted diagnosis certainty: 2.46%  | Actual outcome:  Patient has CHD\n",
            "Predicted diagnosis certainty: 91.62%  | Actual outcome:  Patient doesn't have CHD\n",
            "Predicted diagnosis certainty: 3.35%  | Actual outcome:  Patient has CHD\n",
            "Predicted diagnosis certainty: 99.16%  | Actual outcome:  Patient doesn't have CHD\n",
            "Predicted diagnosis certainty: 0.03%  | Actual outcome:  Patient doesn't have CHD\n",
            "Predicted diagnosis certainty: 95.79%  | Actual outcome:  Patient has CHD\n",
            "Predicted diagnosis certainty: 0.54%  | Actual outcome:  Patient has CHD\n",
            "Predicted diagnosis certainty: 99.98%  | Actual outcome:  Patient doesn't have CHD\n",
            "Predicted diagnosis certainty: 0.21%  | Actual outcome:  Patient doesn't have CHD\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}